{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SparkBasicsTask\n",
    "> 1. Run the first cell to start the Spark Server\n",
    "> 2. Run the second cell to excecute the wordcount code\n",
    "> 3. Go to the SparkUI Url. How many job and stages are there and why?\n",
    "> 4. Copy/paste te wordcount code in a new cell.\n",
    "> 5. Add lines of code after the reduceByKey to also save the counts of the first letter of the word. The wordCount and letterCount must be saved seperately.\n",
    "> _Tip: Python key values are represented by Tuples. To change the (word, count) tuple to (letter,count) you kan use a map function with (wordTuple[0][0],wordTuple[1]). The first [0] represents the first element of the tuple (the key). The second [0] is the first letter of the key string)\n",
    "> 6. Go to the SparkUI Url. How many jobs are created? Why? Inspect the DAG of the 2 last jobs.\n",
    "\n",
    "> _The engine creates a job for every \"action\" in the code. For every action it will create a new excecution plan and all tranformations will be processed twice._\n",
    "> 7. Go to the Storage tab. Notice the tab is empty.\n",
    "> 8. To avoid Spark from excecuting all the steps twice add wordCounts.cache() after the reduceByKey.\n",
    "> 9. Run the code again. How many jobs are created now? Why? Inspect the DAG of the 2 last jobs. What is the difference with the previous DAGS?\n",
    "> 10. Go to the Storage tab. Notice the tab is not empty anymore. Inspect the storage tab.\n",
    "> _By enabling caching, the first job will store the intermediate result. The second job will reuse this result, which will prevent processing the same steps twice. With caching, you can choose between processing and memory. When enough memory is available you can greatly increase the performance of the jobs._\n",
    "> 11. Excecute sc.stop() to stop the Spark Server\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'http://AKDGPORT11191.mshome.net:4040'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "#Make sure pyspark is installed as a package of your project.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "conf = SparkConf().setAppName(\"firstJob\").setMaster(\"local[*]\").setIfMissing(\"spark.logLineage\", \"true\")\n",
    "sc =SparkContext.getOrCreate((conf))\n",
    "sc.uiWebUrl"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T11:53:58.650620Z",
     "end_time": "2023-08-31T11:53:58.930943Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "lines = sc.textFile(\"./FileStore/tables/shakespeare.txt\")\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter(lambda word: len(word) > 0)\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "print(wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T11:54:00.158584Z",
     "end_time": "2023-08-31T11:54:04.005565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "PythonRDD[58] at RDD at PythonRDD.scala:53"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "lines = sc.textFile(\"./FileStore/tables/shakespeare.txt\")\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).filter(lambda word: len(word) > 0)\n",
    "wordKv = words.map(lambda word: (word, 1))\n",
    "wordCounts = wordKv.reduceByKey(lambda a,b:a +b)\n",
    "#print(wordCounts.first())\n",
    "wordCounts.cache()\n",
    "letterCounts = wordCounts.map(lambda wordTuple: (wordTuple[0][0],wordTuple[1]) )\n",
    "letterCounts = letterCounts.reduceByKey(lambda a,b:a +b)\n",
    "print(wordCounts.saveAsTextFile(\"./output/words\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")))\n",
    "print(letterCounts.saveAsTextFile(\"./output/letters\" + datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T12:13:22.618403Z",
     "end_time": "2023-08-31T12:13:31.660529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "sc.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T11:53:50.930369Z",
     "end_time": "2023-08-31T11:53:51.373684Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
