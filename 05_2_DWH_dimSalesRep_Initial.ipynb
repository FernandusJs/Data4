{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T10:08:28.671963Z",
     "end_time": "2023-09-01T10:08:29.092162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"dimSalesRepInit\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T10:09:07.058926Z",
     "end_time": "2023-09-01T10:10:24.879971Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial load\n",
    "We will create a slowly changing dimension type 2 called dimSalesRep based on a sourceTable in our operational database called dbo.salesrep. SCD2  tables demand extra care and  because we will store hirstorical values of the dimension with the help of ranges.\n",
    "This notebook will create the table and fill it with the initial data. A second notebook will be used for increments of new and changed data.\n",
    "\n",
    "This is an example of the expected output (salesRepSK is different\n",
    "```\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|salesRepID|         name|       office| salesRepSK|          scd_start|            scd_end|                 md5|current|\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|a46add1...|      Z. Jane|     New York|          0|1990-01-01 00:00:00|2100-12-12 00:00:00|303db545462092a92...|   true|\n",
    "|s1fedf1...|   P. Chapman|       Berlin|          1|1990-01-01 00:00:00|2100-12-12 00:00:00|14b094c31bf9e4149...|   true|\n",
    "|d5e6f77...|     T. Crane|     New York|          2|1990-01-01 00:00:00|2100-12-12 00:00:00|6c062f95defda9dc3...|   true|\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.set_connection(\"tutorial_op\")\n",
    "\n",
    "df_operational_sales_rep = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"dbo.salesrep\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"salesRepID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "#Method 1 : Use the DataFrame API\n",
    "#df_dim_sales_rep = df_sales_rep.withColumn(\"salesRepSK\", expr(\"uuid()\")) \\\n",
    "#    .withColumn(\"scd_start\", lit(\"1990-01-01\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"scd_end\", lit(\"2100-12-12\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"md5\", md5(concat( col('name'), col('office')))) \\\n",
    "#    .withColumn(\"current\", lit(True))\n",
    "\n",
    "#Method 2 : Use SQL\n",
    "df_operational_sales_rep.createOrReplaceTempView(\"dimSalesRep\")\n",
    "df_dim_sales_rep = spark.sql(\"select uuid() as salesRepSK, *, to_timestamp('1999-01-01','yyyy-MM-dd') as scd_start, to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end, md5(concat( name, office)) as md5, True as current  from dimSalesRep\")\n",
    "\n",
    "df_dim_sales_rep.printSchema()\n",
    "df_dim_sales_rep.show()\n",
    "df_dim_sales_rep.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimSalesRep\")\n",
    "\n",
    "#spark.sql(\"ALTER TABLE dimSalesRep  ADD columns (salesRepSK long GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T10:40:02.328129Z",
     "end_time": "2023-09-01T10:40:08.238681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The function lit() is used when you want a fixed column value for every row. In this case scd_start, scd_end and current.\n",
    "* The function md5() performs a md5-hash function on the preferred columns. This is needed to detect scd2 changes. When a specific column changes, the md5-hash will change. Include all SCD2 columns in the md5-hash function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T10:40:49.223259Z",
     "end_time": "2023-09-01T10:40:49.450587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
