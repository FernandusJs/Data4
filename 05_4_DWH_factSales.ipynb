{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1f752c6ddf0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.admin.kdg.be:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>FactSales</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"FactSales\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \":\".join(cc.jars)) \\\n",
    "    .master(\"local[*]\")\n",
    "builder = configure_spark_with_delta_pip(builder)\n",
    "spark = builder.getOrCreate()\n",
    "builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fact transformations\n",
    "This notebooks creates the sales fact table from scratch based on the operational source table \"sales\"\n",
    "When creating a fact table always follow the listed steps in order."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1 READ NECESSARY SOURCE TABLE(S) AND PERFORM TRANSFORMATIONS\n",
    "**When reading from the source table make sure you include all data necessary:**\n",
    "- to calculate the measure values\n",
    "- the source table keys that you have to use to lookup the correct surrogate keys in the dimension tables.\n",
    "\n",
    "**If more than one table is needed to gather the necesary information you can opt for one of two strategies:**\n",
    "- Use a select query when reading from the jdbc source with the spark.read operation. Avoid complex queries because the operational database needs a lot of resources to run those queries.\n",
    "- Perform a spark.read operation for each table separately and join the tables within Spark. The joins will take place on the cluster instead of the database. You limit the database recources used, but there can be a significant overhead of unnecessary data tranferred to the cluster.\n",
    "\n",
    "\n",
    "In this case we just rename Amount and create a default count_mv column.\n",
    "*In this case, the transformations are minimal.\n",
    "In reality, those transformations can be far more complex. If so, it can be advisable to work out the transforms in more then one step.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "cc.set_connection(\"mydb\")\n",
    "sale_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"dbo.sales\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"Order_ID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\\\n",
    "    .withColumnRenamed(\"Amount\",\"revenue_mv\")\\\n",
    "    .withColumn(\"count_mv\", lit(1))\n",
    "\n",
    "#sale_src_df.show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 2 MAKE DIMENSION TABLES AVAILABLE AS VIEWS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "dim_salesrep = spark.read.format(\"delta\").load(\"spark-warehouse/dimsalesrep/\")\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "dim_salesrep.createOrReplaceTempView(\"dimSalesRep\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3 Build the fact table\n",
    "\n",
    "Within the creation of a fact table always perform these two tasks:\n",
    "1.   Include the measures of the fact\n",
    "2. Use the dimension tables to look up the surrogate keys that correspond with the natural key value. In case of SCD2 dimension use the scd_start en scd_end to find the correct version of the data in the dimension\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-------+----------+\n",
      "|OrderID|dateSK|salesrepSK|countMV|revenue_mv|\n",
      "+-------+------+----------+-------+----------+\n",
      "|     57|   866|         1|      1| 778297706|\n",
      "|     58|   320|         1|      1|2100696115|\n",
      "|     59|  1012|         1|      1| 642114638|\n",
      "|     60|  1375|         1|      1|1440206513|\n",
      "|     61|   954|         1|      1|1244596895|\n",
      "|     62|   957|         1|      1|1662557955|\n",
      "|     63|    62|         1|      1| 844183988|\n",
      "|     64|   229|         1|      1| 681975050|\n",
      "|     65|   684|         1|      1|1504576144|\n",
      "|     66|   876|         1|      1| 622992265|\n",
      "+-------+------+----------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_src_df.createOrReplaceTempView(\"sales_source\")\n",
    "salesFactFromSource = spark.sql(\"select src.Order_ID as OrderID, dd.dateSk as dateSK, ds.salesrepSK, src.count_mv as countMV, src.revenue_mv as revenue_mv \\\n",
    "          from sales_source as src \\\n",
    "          left outer join dimdate as dd on src.Order_Date = cast(dd.CalendarDate as DATE) \\\n",
    "           left outer join dimSalesRep as ds \\\n",
    "                      on src.SalesRepID = ds.SalesRepID \\\n",
    "                      and src.Order_Date > ds.scd_start \\\n",
    "                      and src.Order_Date <= ds.scd_end\")\n",
    "\n",
    "salesFactFromSource.show(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial load\n",
    "The first time loading the fact table perform a FULL load. All data is written to the Delta Table."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "#salesFactFromSource.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factSales\")\n",
    "\n",
    "\n",
    "#spark.sql(\"ALTER TABLE dimSalesRep  ADD columns (salesRepSK long GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "When previous runs where performend you can opt for a 'faster' icremental run that only writes away changes.\n",
    "In this example two merges are used: One that updates existing records and one that inserts new records.\n",
    "\n",
    "\n",
    "- *Why two merges? Combining them in one merge did result in creating copies of all parquet files. This is due to the Delta Lake internals*\n",
    "- * In our solution we detect a change by comparing avery source field with the existing fact field. Using the md5 hash strategy (see SCD2 dimension) would have been a superior solution. The MD5 column can be added in step 3.\n",
    "- * To make the loading proces even faster you can opt to only read changed rows from the source tables (sales operational table in this case). This however requires a \"last_updated\" timestamp in the source table.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "fact = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factsales\")\n",
    "#Merge to perform updates (TODO: Implement md5 strategy)\n",
    "fact.alias(\"current\")\\\n",
    ".merge(salesFactFromSource.alias(\"new\"),\n",
    "       \"current.OrderID = new.OrderId \"\n",
    "       \"and (current.dateSK <> new.dateSK OR current.salesrepSK <> new.salesrepSK OR current.countMV <> new.countMV OR current.revenue_MV <> new.revenue_MV)\").whenMatchedUpdateAll().execute()\n",
    "#Merge to perform inserts\n",
    "fact.alias(\"current\").merge(salesFactFromSource.alias(\"new\"),\"current.OrderID = new.OrderId\").whenNotMatchedInsertAll().execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n",
      "+-------+------+----------+-------+----------+\n",
      "|OrderID|dateSK|salesrepSK|countMV|revenue_mv|\n",
      "+-------+------+----------+-------+----------+\n",
      "|      1|  5405|         0|      1|        10|\n",
      "+-------+------+----------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'selec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-17-ea69ebe1c8dd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"select * from factsales where orderId=1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mselec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'selec' is not defined"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: ALWAYS TEST THE CREATED CODE.\n",
    "# In this example I changed order 498 in the operational database and checked the change after the run.\n",
    "# spark.sql(\"select * from factsales f join dimsalesrep ds on f.salesrepSK = ds.salesrepSK where OrderID = 192  \").show()\n",
    "spark.sql(\"select count(*) from factsales\").show()\n",
    "spark.sql(\"select * from factsales where orderId=1\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the history of your delta fact table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\n",
    "\n",
    "fact.history().show(10,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}