{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T13:39:33.464800Z",
     "end_time": "2023-09-01T13:39:34.087111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"dimSalesIncrementalLoad\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T13:48:10.072349Z",
     "end_time": "2023-09-01T13:48:52.709787Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "\n",
    "After the sales Rep dimension is filled for the first time, the logic to update the dimension has to be handled differently. A change of a record in the source system has to be handled as a change in the dimension. The SCD2 logic is used to handle this.\n",
    "\n",
    "The SCD2 implementation requires a more complex transformation to correctly handle changes in the source files. For detailed information consult the comments in the code.\n",
    "### Setting the parameters\n",
    "The timestamp of the job is used to set the scd_end date of the previous record and the scd_start date of the new record."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_timestamp =datetime.now() #The job runtime is stored in a variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read source table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1. READ EXISTING DIMENSION\n",
    "Read the existing deltaTable (as a deltaTable object, not a Dataframe). Make the table available as a View."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt_dimSalesRep = DeltaTable.forPath(spark,\".\\spark-warehouse\\dimsalesrep\")\n",
    "\n",
    "dt_dimSalesRep.toDF().filter('current = true').createOrReplaceTempView(\"dimSalesRep_current\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DIMENSION\n",
    "#spark.sql(\"select salesRepID, name, office, salesRepSK, md5  from dimSalesRep_current \").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T14:18:57.387718Z",
     "end_time": "2023-09-01T14:18:58.073875Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 2 READ SOURCE TABLE\n",
    "Creating dataframe with source table (from operational system). Transformed to the dimension format.\n",
    "The surrogate key is a uuid to be sure it's unique.\n",
    "md5 hash is used to identify changes in the source table.\n",
    "A view is created of the resulting dataframe to make it available for the next step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.set_connection(\"tutorial_op\")\n",
    "\n",
    "#a. Reading from a JDBC source\n",
    "df_operational_sales_rep = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"dbo.salesrep\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"salesRepID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "df_operational_sales_rep.createOrReplaceTempView(\"operational_sales_rep\")\n",
    "\n",
    "#b. Transforming the source to the dimension format\n",
    "df_dim_sales_rep_new = spark.sql( \"select uuid() as source_salesRepSK, \\\n",
    "                                        salesRepId as source_salesRepId, \\\n",
    "                                        name as source_name, \\\n",
    "                                        office as source_office, \\\n",
    "                                        md5(concat( name, office)) as source_md5 \\\n",
    "                                    from operational_sales_rep\")\n",
    "\n",
    "df_dim_sales_rep_new.createOrReplaceTempView(\"dimSalesRep_new\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF SOURCE\n",
    "#df_dim_sales_rep_new.printSchema()\n",
    "#df_dim_sales_rep_new.show()\n",
    "#spark.sql(\"select * from dimSalesRep_new\").show()\n",
    "#df_dim_sales_rep.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimSalesRep\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T13:49:39.701217Z",
     "end_time": "2023-09-01T13:49:41.585500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 3 DETECT CHANGES\n",
    "Dataframe to identify SCD2 changed rows.\n",
    "First a join between SOURCE (operational system) and DIMENSION (dwh) is performed\n",
    "   The md5 hash is used to identify differences.\n",
    "   The list contains:\n",
    "       - updated source-rows (the join finds a rowand the md5 is different)  and\n",
    "       - new source-rows (the leftouter join does not find a row in the dimension (dwh.salesRepId is null)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "detectedChanges=spark.sql(f\"select * \\\n",
    "                          from dimSalesRep_new source \\\n",
    "                          left outer join dimSalesRep_current dwh on dwh.salesRepID == source.source_salesRepId and dwh.current == true \\\n",
    "                          where dwh.salesRepId is null or dwh.md5 <> source.source_md5\")\n",
    "\n",
    "detectedChanges.createOrReplaceTempView(\"detectedChanges\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DETECTED CHANGES\n",
    "#detectedChanges.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T13:49:54.779289Z",
     "end_time": "2023-09-01T13:49:56.269778Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 4 TRANSOFRM TO UPSERTS\n",
    "Every updated and new source-row requires the insertion of a new record in the SCD2 dimension.\n",
    "Updated source-rows also require an update of the existing scd-fields.\n",
    "\n",
    "Rows without mergeKey will be inserted in the dimension table.\n",
    "Rows with mergekey will be updated in the dimension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_upserts = spark.sql(f\"select null as mergeKey, \\\n",
    "                                source_salesRepSK as salesRepSK,\\\n",
    "                                source_salesRepId as salesRepId,\\\n",
    "                                source_name as name,\\\n",
    "                                source_office as office,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_start, \\\n",
    "                                to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end,\\\n",
    "                                source_md5 as md5,\\\n",
    "                                true as current\\\n",
    "                        from  detectedChanges\\\n",
    "                        union \\\n",
    "                        select salesRepId as mergeKey,\\\n",
    "                                salesRepSK,\\\n",
    "                                salesRepId,\\\n",
    "                                name,\\\n",
    "                                office,\\\n",
    "                                scd_start,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_end,\\\n",
    "                                md5, \\\n",
    "                                false \\\n",
    "                                from detectedChanges \\\n",
    "                        where current is not null\")\n",
    "\n",
    "df_upserts.createOrReplaceTempView(\"upserts\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T14:11:14.163596Z",
     "end_time": "2023-09-01T14:11:14.288978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 5 PERFORM MERGE DIMSALESREP AND UPSERTS\n",
    "merge looks for a matching dwh.salesRepID (in the dimension) for mergeKey\n",
    "   - when a match is found (the dimension table contains a row where its salesRepId corresponds with one of the mergekeys)  -> perform update of row to close the period and set current to \"false\"\n",
    "   - when no match is found (there is no salesRepID in the dimension because the mergeKey is null) -> perform an insert with the data from the updserts table (from the source). The scd-start is filled with the run_timestamp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.sql(\"MERGE INTO dimSalesRep_current AS target \\\n",
    "          using upserts AS source ON target.salesRepID = source.mergeKey \\\n",
    "          WHEN MATCHED THEN UPDATE SET * \\\n",
    "          WHEN NOT MATCHED THEN INSERT *\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DIMENSION\n",
    "dt_dimSalesRep.toDF().sort(\"salesRepID\", \"scd_start\").show(100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-01T14:11:19.662140Z",
     "end_time": "2023-09-01T14:11:32.175372Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
