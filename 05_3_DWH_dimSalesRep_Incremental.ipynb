{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:13:08.499376Z",
     "end_time": "2023-09-20T13:13:08.529659Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"dimSalesIncrementalLoad\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:13:10.908116Z",
     "end_time": "2023-09-20T13:13:11.090306Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "\n",
    "After the sales Rep dimension is filled for the first time, the logic to update the dimension has to be handled differently. A change of a record in the source system has to be handled as a change in the dimension. The SCD2 logic is used to handle this.\n",
    "\n",
    "The SCD2 implementation requires a more complex transformation to correctly handle changes in the source files. For detailed information consult the comments in the code.\n",
    "### Setting the parameters\n",
    "The timestamp of the job is used to set the scd_end date of the previous record and the scd_start date of the new record."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_timestamp =datetime.now() #The job runtime is stored in a variable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:13:14.655965Z",
     "end_time": "2023-09-20T13:13:14.687211Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read existing dimension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt_dimSalesRep = DeltaTable.forPath(spark,\".\\spark-warehouse\\dimsalesrep\")\n",
    "\n",
    "dt_dimSalesRep.toDF().createOrReplaceTempView(\"dimSalesRep_current\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DIMENSION\n",
    "spark.sql(\"select salesRepID, name, office, salesRepSK, md5  from dimSalesRep_current \").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:14:46.562779Z",
     "end_time": "2023-09-20T13:14:52.302594Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read source table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 1 READ SOURCE TABLE\n",
    "Creating dataframe with source table (from operational system). Transformed to the dimension format.\n",
    "The surrogate key is a uuid to be sure it's unique.\n",
    "md5 hash is used to identify changes in the source table.\n",
    "A view is created of the resulting dataframe to make it available for the next step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc.set_connection(\"tutorial_op\")\n",
    "\n",
    "#a. Reading from a JDBC source\n",
    "df_operational_sales_rep = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"dbo.salesrep\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"salesRepID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "df_operational_sales_rep.createOrReplaceTempView(\"operational_sales_rep\")\n",
    "\n",
    "#b. Transforming the source to the dimension format\n",
    "df_dim_sales_rep_new = spark.sql( \"select uuid() as source_salesRepSK, \\\n",
    "                                        salesRepId as source_salesRepId, \\\n",
    "                                        name as source_name, \\\n",
    "                                        office as source_office, \\\n",
    "                                        md5(concat( name, office)) as source_md5 \\\n",
    "                                    from operational_sales_rep\")\n",
    "\n",
    "df_dim_sales_rep_new.createOrReplaceTempView(\"dimSalesRep_new\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF SOURCE\n",
    "#df_dim_sales_rep_new.printSchema()\n",
    "#df_dim_sales_rep_new.show()\n",
    "spark.sql(\"select * from dimSalesRep_new\").show()\n",
    "#df_dim_sales_rep.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimSalesRep\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:14:54.082875Z",
     "end_time": "2023-09-20T13:14:54.368902Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 2 DETECT CHANGES\n",
    "Dataframe to identify SCD2 changed rows.\n",
    "First a join between SOURCE (operational system) and DIMENSION (dwh) is performed\n",
    "   The md5 hash is used to identify differences.\n",
    "   The list contains:\n",
    "       - updated source-rows (the join finds a rowand the md5 is different)  and\n",
    "       - new source-rows (the leftouter join does not find a row in the dimension (dwh.salesRepId is null)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "detectedChanges=spark.sql(f\"select * \\\n",
    "                          from dimSalesRep_new source \\\n",
    "                          left outer join dimSalesRep_current dwh on dwh.salesRepID == source.source_salesRepId and dwh.current == true \\\n",
    "                          where dwh.salesRepId is null or dwh.md5 <> source.source_md5\")\n",
    "\n",
    "detectedChanges.createOrReplaceTempView(\"detectedChanges\")\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DETECTED CHANGES\n",
    "detectedChanges.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:15:02.218224Z",
     "end_time": "2023-09-20T13:15:03.003956Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### 3 TRANSOFRM TO UPSERTS\n",
    "Before union: Every updated and new source-row requires the insertion of a new record in the SCD2 dimension. This new records starts at the runtime of the job and ends at the end of time (2100-12-12). Current is set to true.\n",
    "Updated source-rows also require an update of the existing scd-fields. The scd_end date of the existing record is set to the runtime of the job. Current is set to false\n",
    "\n",
    "In the next step, rows without mergeKey will be inserted in the dimension table and rows with mergekey will be updated in the dimension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_upserts = spark.sql(f\"select source_salesRepSK as salesRepSK,\\\n",
    "                                source_salesRepId as salesRepID,\\\n",
    "                                source_name as name,\\\n",
    "                                source_office as office,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_start, \\\n",
    "                                to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end,\\\n",
    "                                source_md5 as md5,\\\n",
    "                                true as current\\\n",
    "                        from  detectedChanges\\\n",
    "                        union \\\n",
    "                        select  salesRepSK,\\\n",
    "                                salesRepId,\\\n",
    "                                name,\\\n",
    "                                office,\\\n",
    "                                scd_start,\\\n",
    "                                to_timestamp('{run_timestamp}') as scd_end,\\\n",
    "                                md5, \\\n",
    "                                false \\\n",
    "                                from detectedChanges \\\n",
    "                        where current is not null\")\n",
    "\n",
    "df_upserts.createOrReplaceTempView(\"upserts\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:16:00.672450Z",
     "end_time": "2023-09-20T13:16:00.807034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF UPSERTS\n",
    "spark.sql(\"select * from upserts\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:16:11.462636Z",
     "end_time": "2023-09-20T13:16:13.801576Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### PERFORM MERGE DIMSALESREP AND UPSERTS\n",
    "merge looks for a matching dwh.salesRepID (in the dimension) for mergeKey\n",
    "   - when a match is found (the dimension table contains a row where its salesRepId corresponds with one of the mergekeys)  -> perform update of row to close the period and set current to \"false\"\n",
    "   - when no match is found (there is no salesRepID in the dimension because the mergeKey is null) -> perform an insert with the data from the updserts table (from the source). The scd-start is filled with the run_timestamp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.sql(\"MERGE INTO dimSalesRep_current AS target \\\n",
    "          using upserts AS source ON target.salesRepID = source.salesRepID and source.current = false and target.current=true \\\n",
    "          WHEN MATCHED THEN UPDATE SET scd_end = source.scd_end, current = source.current  \\\n",
    "          WHEN NOT MATCHED THEN INSERT (salesRepSK, salesRepId, name, office, scd_start, scd_end, md5, current) values (source.salesRepSK, source.salesRepId, source.name, source.office, source.scd_start, source.scd_end, source.md5, source.current)\")\\\n",
    "\n",
    "#DEBUG CODE TO SHOW CONTENT OF DIMENSION\n",
    "dt_dimSalesRep.toDF().sort(\"salesRepID\", \"scd_start\").show(100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:17:21.291860Z",
     "end_time": "2023-09-20T13:17:25.580972Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
