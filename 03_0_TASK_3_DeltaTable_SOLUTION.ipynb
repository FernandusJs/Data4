{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002049E5BB880>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x2049e5bb880>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.mshome.net:4042\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DeltaTableEx</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"DeltaTableEx\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T15:48:09.565562Z",
     "end_time": "2023-08-31T15:49:13.848969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "# Step 1: Load the dataset into a Delta table\n",
    "transaction_data_path = \"./FileStore/tables/transactions.csv\"  # Replace with the actual path to the transaction data CSV file\n",
    "transaction_delta_path = \"./spark-warehouse/transaction_data_delta\"  # Replace with the actual path where you want to store the Delta table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:36.832259Z",
     "end_time": "2023-08-31T16:15:36.877285Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Persist a detlatable to disk based on the CSV transactions.csv\n",
    "Read the CSV file transaction_data_path\n",
    "Write it as a Delta table to transaction_delta_path\n",
    "Use inferSchema option (see: https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/) to automatically infer the schema from the CSV file. Otherwise all columns will be of type string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "# Read the CSV data and write it as a Delta table\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(transaction_data_path)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(transaction_delta_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:38.490318Z",
     "end_time": "2023-08-31T16:15:42.695291Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a DeltaTable object for the persisted transaction data\n",
    "Use DetlaTable.forPath() to create a DeltaTable object for the persisted transaction data\n",
    "A delta table object is needed to use functions like history() and vacuum()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, transaction_delta_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:44.839437Z",
     "end_time": "2023-08-31T16:15:44.923467Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the schema and history information for the Delta table\n",
    "To get the schema get the dataframe object with toDf() and use printSchema()\n",
    "Delta table has a history() method that returns a dataframe with the history of the delta table\n",
    "Make sure you understand the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "|format|                  id|name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|       tableFeatures|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "| delta|0d8611c6-9787-4bc...|null|       null|file:/C:/DevProje...|2023-08-31 16:15:...|2023-08-31 16:15:...|              []|       1|       1740|        {}|               1|               2|[appendOnly, inva...|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2023-08-31 16:15:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.4....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the # Ge schema and history\n",
    "delta_table.toDF().printSchema()\n",
    "delta_table.detail().show()\n",
    "delta_table.history().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:46.931139Z",
     "end_time": "2023-08-31T16:15:47.308137Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a query on the delta table to find the total number of transactions\n",
    "Use toDF() to convert the delta table to a spark dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of transactions: 10\n"
     ]
    }
   ],
   "source": [
    "total_transactions = delta_table.toDF().count()\n",
    "print(f\"Total number of transactions: {total_transactions}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:51.274798Z",
     "end_time": "2023-08-31T16:15:52.415799Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a view on the delta table with the name 'transactions'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Create a view on the delta table\n",
    "delta_table.toDF().createOrReplaceTempView(\"transactions\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:54.443612Z",
     "end_time": "2023-08-31T16:15:54.485652Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update the delta table to increase the quantity of a specific product by a given value\n",
    "You canw write an update statement on the delta table with spark.sql(). This is not supported without the use of delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Update the Delta table to increase the quantity of a specific product by a given value spark.sql\n",
    "spark.sql(\"UPDATE transactions SET quantity = quantity + 1 WHERE product_name = 'Product A'\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:15:56.699535Z",
     "end_time": "2023-08-31T16:16:02.374530Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table again\n",
    "Try to understand what you see\n",
    "When the results are truncated, you can use the vertical option in method show() to see the full results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-08-31 16:15:58.204                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | UPDATE                                                                                                                                                                                                                           \n",
      " operationParameters | {predicate -> [\"(product_name#3894 = Product A)\"]}                                                                                                                                                                               \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | 0                                                                                                                                                                                                                                \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numRemovedFiles -> 1, numRemovedBytes -> 1740, numCopiedRows -> 6, numAddedChangeFiles -> 0, executionTimeMs -> 1446, scanTimeMs -> 1050, numAddedFiles -> 1, numUpdatedRows -> 4, numAddedBytes -> 1734, rewriteTimeMs -> 392} \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-08-31 16:15:39.118                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | WRITE                                                                                                                                                                                                                            \n",
      " operationParameters | {mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                           \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | null                                                                                                                                                                                                                             \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1740}                                                                                                                                                                     \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show(truncate = False, vertical=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:16:05.823898Z",
     "end_time": "2023-08-31T16:16:06.229881Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delete all transactions from customer 101\n",
    "You canw write an delete statement on the delta table with spark.sql(). This is not supported without the use of delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                3|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete all transactions from customer 101\n",
    "spark.sql(\"DELETE from transactions where customer_id=101\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:16:11.378509Z",
     "end_time": "2023-08-31T16:16:16.154913Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table again to see what is changed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             2|        102|   2023-06-02|   Product B|       5|\n",
      "|             3|        103|   2023-06-03|   Product A|       4|\n",
      "|             5|        104|   2023-06-05|   Product B|       4|\n",
      "|             6|        102|   2023-06-06|   Product A|       3|\n",
      "|             7|        103|   2023-06-07|   Product C|       3|\n",
      "|             9|        104|   2023-06-09|   Product A|       3|\n",
      "|            10|        103|   2023-06-10|   Product B|       3|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-08-31 16:16:12.722                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | DELETE                                                                                                                                                                                                                           \n",
      " operationParameters | {predicate -> [\"(customer_id#3892 = 101)\"]}                                                                                                                                                                                      \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | 1                                                                                                                                                                                                                                \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numRemovedFiles -> 1, numRemovedBytes -> 1734, numCopiedRows -> 7, numAddedChangeFiles -> 0, executionTimeMs -> 1273, numDeletedRows -> 3, scanTimeMs -> 866, numAddedFiles -> 1, numAddedBytes -> 1688, rewriteTimeMs -> 406}  \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-08-31 16:15:58.204                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | UPDATE                                                                                                                                                                                                                           \n",
      " operationParameters | {predicate -> [\"(product_name#3894 = Product A)\"]}                                                                                                                                                                               \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | 0                                                                                                                                                                                                                                \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numRemovedFiles -> 1, numRemovedBytes -> 1740, numCopiedRows -> 6, numAddedChangeFiles -> 0, executionTimeMs -> 1446, scanTimeMs -> 1050, numAddedFiles -> 1, numUpdatedRows -> 4, numAddedBytes -> 1734, rewriteTimeMs -> 392} \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-08-31 16:15:39.118                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | WRITE                                                                                                                                                                                                                            \n",
      " operationParameters | {mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                           \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | null                                                                                                                                                                                                                             \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1740}                                                                                                                                                                     \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transactions\").show()\n",
    "delta_table.history().show(truncate=False,vertical=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:16:16.124909Z",
     "end_time": "2023-08-31T16:16:17.878894Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform an merge operation\n",
    "Get info on merge operations: https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&language-sql\n",
    "1. Create a new dataframe original_df that reads the original transaction data from the CSV file and create a temporary view 'original_transactions'\n",
    "2. Perform a merge of  'original_transactions' into 'transactions'\n",
    "3. When a match is found, update the quantity for that row to 0\n",
    "4. When no match is found, insert the row from the new dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Perform an upsert\n",
    "original_df =spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(transaction_data_path)\n",
    "original_df.createOrReplaceTempView(\"original_transactions\")\n",
    "\n",
    "spark.sql(\"MERGE INTO transactions AS target \\\n",
    "          using original_transactions AS source ON target.transaction_id = source.transaction_id \\\n",
    "          WHEN MATCHED THEN UPDATE SET quantity = 0 \\\n",
    "          WHEN NOT MATCHED THEN INSERT *\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:16:24.377661Z",
     "end_time": "2023-08-31T16:16:28.956780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Show the detla table and detla table history\n",
    "Is the result what you expected?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             1|        101|   2023-06-01|   Product A|       2|\n",
      "|             2|        102|   2023-06-02|   Product B|       0|\n",
      "|             3|        103|   2023-06-03|   Product A|       0|\n",
      "|             4|        101|   2023-06-04|   Product C|       1|\n",
      "|             5|        104|   2023-06-05|   Product B|       0|\n",
      "|             6|        102|   2023-06-06|   Product A|       0|\n",
      "|             7|        103|   2023-06-07|   Product C|       0|\n",
      "|             8|        101|   2023-06-08|   Product B|       5|\n",
      "|             9|        104|   2023-06-09|   Product A|       0|\n",
      "|            10|        103|   2023-06-10|   Product B|       0|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n",
      "-RECORD 0-----------------------------------\n",
      " version             | 3                    \n",
      " timestamp           | 2023-08-31 16:16:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | MERGE                \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 2                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numTargetRowsCop... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 1-----------------------------------\n",
      " version             | 2                    \n",
      " timestamp           | 2023-08-31 16:16:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | DELETE               \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 1                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numRemovedFiles ... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 2-----------------------------------\n",
      " version             | 1                    \n",
      " timestamp           | 2023-08-31 16:15:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | UPDATE               \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 0                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numRemovedFiles ... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 3-----------------------------------\n",
      " version             | 0                    \n",
      " timestamp           | 2023-08-31 16:15:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | WRITE                \n",
      " operationParameters | {mode -> Overwrit... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | null                 \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numFiles -> 1, n... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delta_table.toDF().show()\n",
    "delta_table.history().show(vertical=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T16:16:35.998994Z",
     "end_time": "2023-08-31T16:16:37.012537Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform a select on the delta table on the second version\n",
    "Use option versionAsOf when reading (spark.read) the delta table from disk\n",
    "You can also travel back in time with option timestampAsOf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             2|        102|   2023-06-02|   Product B|       5|\n",
      "|             3|        103|   2023-06-03|   Product A|       4|\n",
      "|             5|        104|   2023-06-05|   Product B|       4|\n",
      "|             6|        102|   2023-06-06|   Product A|       3|\n",
      "|             7|        103|   2023-06-07|   Product C|       3|\n",
      "|             9|        104|   2023-06-09|   Product A|       3|\n",
      "|            10|        103|   2023-06-10|   Product B|       3|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"./spark-warehouse/transaction_data_delta\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T15:14:29.919517Z",
     "end_time": "2023-06-21T15:14:32.304491Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
