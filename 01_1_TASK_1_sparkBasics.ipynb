{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SparkBasicsTask\n",
    "> 1. Based on the slide about \"Shuffle operations in Spark\", try to guess how many stages the step \"Running WordCount (the Hello World of distributed processing)\" in the 1_SparkBasics notebook will have.\n",
    "> 2. Run that cell\n",
    "> 3. Go to the SparkUI Url answer following questions:\n",
    "> 4. How many stages are there and why?\n",
    "> 2. What changes in the execution plan when you un/comment wordCounts.cache()?\n",
    "> 3. What changes in the Storage-tab when you un/comment wordCounts.cache()?\n",
    "> 4. Copy the last line in the code and change /words to /words2 in the copied line.\n",
    "> 5.   Run the job without caching.\n",
    "> _You will notice the creation of two identical jobs. The engine creates a job for every \"action\" in the code. For every action it will kcreate a new excecution plan and all tranformations will be processed twice._\n",
    "> 6. Enable \"caching\" again and run the code again. Go to the last job. What is changed?\n",
    "> _By enabling caching, the first job will store the intermediate result. The second job will reuse this result, which will prevent processing the same steps twice. With caching, you can choose between processing and memory. When enough memory is available you can greatly increase the performance of the jobs._\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
