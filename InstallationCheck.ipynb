{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Installation Check\n",
    "You should be able to run all cells to advance with the course.\n",
    "Every cell has some troubleshooting guidelines included"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1 : Importing needed modules\n",
    "Possible error solutions:\n",
    "1. Check if the needed packages are installed correctly (View > Tool windows > Python Packages). If not, run the requirements.txt again.\n",
    "2. If you applied some changes to the environment during this session, restart the PyCharme IDE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2 : Importing needed modules\n",
    "Possible error solutions:\n",
    "1. Make sure the imports in step 1 succeeded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DimDate\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "builder = configure_spark_with_delta_pip(builder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3  : Creating a local spark cluster\n",
    "Possible error solutions:\n",
    "\n",
    "1. Make sure the previous step was executed correctly\n",
    "2. Check your environment variables. HADOOP_HOME, SPARK_HOME and PATH have to be set correctly corresponding the instructions in README.MD. In most cases the error message will give you information on what went wrong.\n",
    "3. Read the error message. If you don't get a clear error message look at Jupyter console (View > Tool windows > Python Packages). The console will give information about the startup proces of the Spark-server\n",
    "4. In Windows, make sure your HADOOP_HOME has winutils.exe in the bin folder. If not see README.MD for clear instructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "spark = builder.getOrCreate()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4  : Checking the server web-url\n",
    "After running this step you will get the url (click on Spark UI) to the Spark server. Check if you can visit the URL\n",
    "\n",
    "Possible error solutions:\n",
    "\n",
    "1. Make sure the previous step was executed correctly\n",
    "2. Check your environment variables. HADOOP_HOME, SPARK_HOME and PATH have to be set correctly corresponding the instructions in README.MD. In most cases the error message will give you information on what went wrong.\n",
    "3. Read the error message. If you don't get a clear error message look at Jupyter console (View > Tool windows > Python Packages). The console will give information about the startup proces of the Spark-server\n",
    "4. In Windows, make sure your HADOOP_HOME has winutils.exe in the bin folder. If not see README.MD for clear instructions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1fa01585df0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.mshome.net:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DimDate</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5  : Reading source into Spark DataFrame\n",
    "\n",
    "Possible error solutions:\n",
    "1. Make sure the file is present in the project at [file_location]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"./FileStore/tables/shakespeare.txt\"\n",
    "file_type = \"text\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type)  \\\n",
    "  .load(file_location)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6  : Creating a view on the source and performing SQL on View\n",
    "This step should not pose any problem if the previous steps where successful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2098376981.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[12], line 2\u001B[1;36m\u001B[0m\n\u001B[1;33m    words = lines.\u001B[0m\n\u001B[1;37m                  ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"lines\")\n",
    "words = lines.\n",
    "lowerwords = spark.sql('select lower(col), count(*) as word from words group by lower(col) order by word desc limit 20')\n",
    "lowerwords.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lowerwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# To do so, choose your table name and uncomment the bottom line.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m permanent_table_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshakespeare_txt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 7\u001B[0m \u001B[43mlowerwords\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(permanent_table_name)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'lowerwords' is not defined"
     ]
    }
   ],
   "source": [
    "# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n",
    "# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n",
    "# To do so, choose your table name and uncomment the bottom line.\n",
    "\n",
    "permanent_table_name = \"shakespeare_txt\"\n",
    "\n",
    "lowerwords.write.format(\"delta\").saveAsTable(permanent_table_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\overvelj\\AppData\\Roaming\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: AKDGPORT11191\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "HADOOP_HOME: C:\\DevApps\\winutils\\winutils-master\\hadoop-3.3.1\\\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\overvelj\n",
      "IDEA_INITIAL_DIRECTORY: C:\\WINDOWS\\system32\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-11.0.8\\\n",
      "LANG: en_US.UTF-8\n",
      "LANGUAGE: \n",
      "LC_ALL: en_US.UTF-8\n",
      "LOCALAPPDATA: C:\\Users\\overvelj\\AppData\\Local\n",
      "LOGONSERVER: \\\\CDCDCADM01\n",
      "NUMBER_OF_PROCESSORS: 8\n",
      "ONEDRIVE: C:\\Users\\overvelj\\OneDrive - Karel de Grote Hogeschool\n",
      "ONEDRIVECOMMERCIAL: C:\\Users\\overvelj\\OneDrive - Karel de Grote Hogeschool\n",
      "OS: Windows_NT\n",
      "PATH: C:\\DevApps\\DeltaSpark\\Scripts;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python39;C:\\Program Files\\Java\\jdk-11.0.8\\\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapathREMOVE;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapathREMOVE;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Azure Data Studio\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files\\Graphviz\\bin;C:\\Program Files\\mongosh\\;C:\\Program Files\\MongoDB\\Server\\5.0\\bin;C:\\DevApps\\VENV309_DS2\\\\Scripts;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\DevApps\\gradle-7.6\\bin;C:\\DevApps\\spark-3.3.2-bin-hadoop3\\\\bin;C:\\DevApps\\winutils\\winutils-master\\hadoop-3.3.1\\\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Amazon\\AWSCLIV2\\;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python310_9\\Scripts\\;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python310_9\\;C:\\Users\\overvelj\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\overvelj\\.dotnet\\tools;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Git\\cmd;C:\\Program Files\\mongosh\\;C:\\Users\\overvelj\\AppData\\Local\\JetBrains\\Toolbox\\scripts\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: 8e0c\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PROMPT: (DeltaSpark) $P$G\n",
      "PSDISTRICT_BOOTIMAGEVERSION: CCM002C6\n",
      "PSDISTRICT_DEPLOYMENTID: CCM00B0B\n",
      "PSDISTRICT_INSTALLATIONDATE: 20200814-15:15:29\n",
      "PSDISTRICT_INSTALLATIONMETHOD: PXE\n",
      "PSDISTRICT_OFFICEINSTALL: clicktorun2016\n",
      "PSDISTRICT_SITECODE: CCM\n",
      "PSDISTRICT_TAAK: domein\n",
      "PSDISTRICT_TATTOOSCRIPTVERSION: 1.4.3\n",
      "PSDISTRICT_TSID: CCM00B0B\n",
      "PSDISTRICT_TSNAME: KdG_W10_1909_Prod_200612.02\n",
      "PSDISTRICT_UNATTEND: win10ent64-nl-BE-Admin.xml\n",
      "PSMODULEPATH: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYSPARK_HADOOP_VERSION: 3\n",
      "SESSIONNAME: Console\n",
      "SPARK_HOME: C:\\DevApps\\spark-3.3.2-bin-hadoop3\\\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\overvelj\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\overvelj\\AppData\\Local\\Temp\n",
      "UATDATA: C:\\WINDOWS\\CCM\\UATData\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77\n",
      "USERDNSDOMAIN: ADMIN.KDG.BE\n",
      "USERDOMAIN: ADMIN\n",
      "USERDOMAIN_ROAMINGPROFILE: ADMIN\n",
      "USERNAME: overvelj\n",
      "USERPROFILE: C:\\Users\\overvelj\n",
      "VIRTUAL_ENV: C:\\DevApps\\DeltaSpark\n",
      "WINDIR: C:\\WINDOWS\n",
      "ZES_ENABLE_SYSMAN: 1\n",
      "_OLD_VIRTUAL_PATH: C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python39;C:\\Program Files\\Java\\jdk-11.0.8\\\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapathREMOVE;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapathREMOVE;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Azure Data Studio\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files\\Graphviz\\bin;C:\\Program Files\\mongosh\\;C:\\Program Files\\MongoDB\\Server\\5.0\\bin;C:\\DevApps\\VENV309_DS2\\\\Scripts;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\DevApps\\gradle-7.6\\bin;C:\\DevApps\\spark-3.3.2-bin-hadoop3\\\\bin;C:\\DevApps\\winutils\\winutils-master\\hadoop-3.3.1\\\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Amazon\\AWSCLIV2\\;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python310_9\\Scripts\\;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Python\\Python310_9\\;C:\\Users\\overvelj\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\overvelj\\.dotnet\\tools;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\overvelj\\AppData\\Local\\Programs\\Git\\cmd;C:\\Program Files\\mongosh\\;C:\\Users\\overvelj\\AppData\\Local\\JetBrains\\Toolbox\\scripts\n",
      "_OLD_VIRTUAL_PROMPT: $P$G\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "JPY_INTERRUPT_EVENT: 1200\n",
      "IPY_INTERRUPT_EVENT: 1200\n",
      "JPY_PARENT_PID: 1208\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "2023-03-15 - DBFS Example",
   "dashboards": [
    {
     "elements": [
      {
       "elementNUID": "aacfae14-033d-4eb6-9b6d-0383779f22e5",
       "dashboardResultIndex": 1,
       "guid": "8da8434a-dd05-40cf-9cc6-2b50abde05ca",
       "resultIndex": null,
       "options": null,
       "position": {
        "x": 0,
        "y": 0,
        "height": 6,
        "width": 12,
        "z": null
       },
       "elementType": "command"
      }
     ],
     "guid": "bd908a91-b8fd-4939-bd45-3bb1251b4ada",
     "layoutOption": {
      "stack": true,
      "grid": true
     },
     "version": "DashboardViewV1",
     "nuid": "9ab85a82-8740-4706-8fb4-7a7320db90a0",
     "origId": 3171653423803714,
     "title": "Words used",
     "width": 1024,
     "globalVars": {}
    }
   ],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 3171653423803700
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
