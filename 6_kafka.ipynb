{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kafka and Spark structured streaming\n",
    "\n",
    "## Kafka server setup\n",
    "Folow the instructions on https://www.baeldung.com/ops/kafka-docker-setup to create a local single node kafka cluster.\n",
    "\n",
    "## Kafka producer\n",
    "Clone the SimpleKafkaProducer available on https://gitlab.com/kdg-ti/dataengineering/kafkaproducer\n",
    "This producer simulates a very simple wordcount with 3 words. The timestamps are randomly lagging. This makes it possible to look at how late arriving data is used.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-16T21:01:49.118414Z",
     "end_time": "2023-05-16T21:01:49.839861Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark session setup\n",
    "Spark needs extra jars to be able to run the jobs. Change the pathlocations for your installation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1f792d26700>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.mshome.net:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Kafka</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Kafka\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.jars\", cc.jars_loc + \"kafka-clients-2.6.0.jar,\" \\\n",
    "                          + cc.jars_loc + \"commons-pool2-2.6.2.jar,\" \\\n",
    "                          + cc.jars_loc + \"mssql-jdbc-10.2.1.jre8.jar,\" \\\n",
    "                          + cc.jars_loc + \"spark-sql-kafka-0-10_2.12-3.1.3.jar,\" \\\n",
    "                          + cc.jars_loc + \"spark-tags_2.12-3.1.3.jar, \" \\\n",
    "                          + cc.jars_loc + \"spark-token-provider-kafka-0-10_2.12-3.1.3.jar\") \\\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "builder = configure_spark_with_delta_pip(builder)\n",
    "spark = builder.getOrCreate()\n",
    "builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-16T21:01:51.886776Z",
     "end_time": "2023-05-16T21:02:19.956364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://AKDGPORT11191.mshome.net:64087/jars/kafka-clients-2.6.0.jar, spark://AKDGPORT11191.mshome.net:64087/jars/commons-pool2-2.6.2.jar, spark://AKDGPORT11191.mshome.net:64087/jars/spark-sql-kafka-0-10_2.12-3.1.3.jar, spark://AKDGPORT11191.mshome.net:64087/jars/mssql-jdbc-10.2.1.jre8.jar, spark://AKDGPORT11191.mshome.net:64087/jars/spark-tags_2.12-3.1.3.jar, spark://AKDGPORT11191.mshome.net:64087/jars/spark-token-provider-kafka-0-10_2.12-3.1.3.jar)\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.sc().listJars())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-16T21:02:44.144953Z",
     "end_time": "2023-05-16T21:02:44.306842Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading from a streaming data source\n",
    "We start a readStream on a kafka data source. The server is the one you are running on docker.\n",
    "The subscribe option is needed to define the Topic we are going to read.\n",
    "\n",
    "The incoming messages contain a key and a value in binary format. Other metadata is included. In this specific case the meta data itself is not used\n",
    "\n",
    "### Transforming the \"value\" column\n",
    "The value column is in binary format. It has to be cast to String first. The producer creates json records. json_tuple is used to convert the json elements to seperate columns in string format.\n",
    "The result is converted to a timestamp, key and value column.\n",
    "\n",
    "### Watermarking\n",
    "A watermark is added based on the timestamp field (this field is the effective date on wich the event occured). The delayThreshold is used to set the allowed delay of arrival of\n",
    "\n",
    "### Aggregating the data\n",
    "the data is grouped by word and time windows. Timewindows can be definded with the window function.\n",
    "\n",
    "### Writing to a sink\n",
    "The \"console\" format is used for debugging. Output mode is \"update\" because it will take until after the watermarking threshold to persist any information. Setting the checkpoint location is needed because spark has to create checkpoints in order to start the job again.\n",
    "Tip: If the job starts giving errors on the checkpoint, you can delete the directory and start over again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"AKDGPORT11191.mshome.net:29092\" ) \\\n",
    "      .option(\"subscribe\", \"demo\") \\\n",
    "      .load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-16T21:03:42.970023Z",
     "end_time": "2023-05-16T21:03:43.191855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- c1: string (nullable = true)\n",
      " |-- c2: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x1f792d13a60>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "#df.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")\n",
    "inter = df.selectExpr(\"json_tuple(CAST(value as STRING), 'timestamp', 'key', 'value')\")\n",
    "inter.printSchema()\n",
    "wordCountEvents= inter.selectExpr(\"to_timestamp(trim('[]' FROM c0),'yyyy,M,d,H,m,s,SSSSSSSSS') as timestamp\", \"c1 as key\", \"c2 as value\").withWatermark(\"timestamp\",'30 seconds')\n",
    "wordCountEvents.printSchema()\n",
    "groupedEvents =wordCountEvents.groupBy(F.window(wordCountEvents.timestamp, \"2 minutes\", \"2 minutes\"), wordCountEvents.key) \\\n",
    " .count()\n",
    "\n",
    "groupedEvents.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")\n",
    "\n",
    "#{\"timestamp\":[2022,10,5,17,26,17,470412000],\"key\":\"Fish\",\"value\":1}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop the stream\n",
    "If you do not stop the stream in code, the job wil keep on running."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T16:49:35.863000Z",
     "end_time": "2023-04-26T16:49:35.975647Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
