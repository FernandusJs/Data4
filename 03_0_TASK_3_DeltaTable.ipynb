{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002A358894040>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x2a358894040>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.admin.kdg.be:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DeltaTableEx</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import EnvironmentSetup as es\n",
    "es.setupEnvironment()\n",
    "import ConnectionConfig as cc\n",
    "spark = cc.startLocalCluster(\"DeltaTableEx\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:54:59.620740Z",
     "end_time": "2023-06-21T14:55:24.661073Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "# Step 1: Load the dataset into a Delta table\n",
    "transaction_data_path = \"./FileStore/tables/transactions.csv\"  # Replace with the actual path to the transaction data CSV file\n",
    "transaction_delta_path = \"./spark-warehouse/transaction_data_delta\"  # Replace with the actual path where you want to store the Delta table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:55:27.784295Z",
     "end_time": "2023-06-21T14:55:27.840967Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Persist a detlatable to disk based on the CSV transactions.csv\n",
    "Use inferSchema option to infer the schema from the CSV file. Otherwise all columns will be of type string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType\n",
    "# Read the CSV data and write it as a Delta table\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(transaction_data_path)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(transaction_delta_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:55:32.260239Z",
     "end_time": "2023-06-21T14:55:48.721120Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a DeltaTable object for the persisted transaction data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, transaction_delta_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:55:50.934552Z",
     "end_time": "2023-06-21T14:55:51.021655Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the schema and history information for the Delta table\n",
    "Make sure you onderstand the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2023-06-21 14:55:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.4....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the # Ge schema and history\n",
    "delta_table.toDF().printSchema()\n",
    "delta_table.history().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:55:52.558076Z",
     "end_time": "2023-06-21T14:55:54.748414Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a query on the delta table to find the total number of transactions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of transactions: 10\n"
     ]
    }
   ],
   "source": [
    "total_transactions = delta_table.toDF().count()\n",
    "print(f\"Total number of transactions: {total_transactions}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:00.267090Z",
     "end_time": "2023-06-21T14:56:02.488993Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a view on the delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Create a view on the delta table\n",
    "delta_table.toDF().createOrReplaceTempView(\"transactions\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:16.147241Z",
     "end_time": "2023-06-21T14:56:16.247454Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update the delta table to increase the quantity of a specific product by a given value\n",
    "This is not supported without the use of delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Update the Delta table to increase the quantity of a specific product by a given value spark.sql\n",
    "spark.sql(\"UPDATE transactions SET quantity = quantity + 1 WHERE product_name = 'Product A'\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:18.566193Z",
     "end_time": "2023-06-21T14:56:25.617060Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table\n",
    "Try to understand what you see\n",
    "When the results are truncated, you can use the vertical option in method show() to see the full results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                              |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2023-06-21 14:56:20.938|null  |null    |UPDATE   |{predicate -> [\"(product_name#776 = Product A)\"]}|null|null    |null     |0          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1740, numCopiedRows -> 6, numAddedChangeFiles -> 0, executionTimeMs -> 2181, scanTimeMs -> 1590, numAddedFiles -> 1, numUpdatedRows -> 4, numAddedBytes -> 1734, rewriteTimeMs -> 583}|null        |Apache-Spark/3.4.0 Delta-Lake/2.4.0|\n",
      "|0      |2023-06-21 14:55:41.709|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}           |null|null    |null     |null       |Serializable  |false        |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1740}                                                                                                                                                                    |null        |Apache-Spark/3.4.0 Delta-Lake/2.4.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show(truncate = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:25.619456Z",
     "end_time": "2023-06-21T14:56:26.017913Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delete all transactions from customer 101"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                3|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete all transactions from customer 101\n",
    "spark.sql(\"DELETE from transactions where customer_id=101\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:35.340747Z",
     "end_time": "2023-06-21T14:56:41.031617Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table again to see what is changed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             2|        102|   2023-06-02|   Product B|       5|\n",
      "|             3|        103|   2023-06-03|   Product A|       4|\n",
      "|             5|        104|   2023-06-05|   Product B|       4|\n",
      "|             6|        102|   2023-06-06|   Product A|       3|\n",
      "|             7|        103|   2023-06-07|   Product C|       3|\n",
      "|             9|        104|   2023-06-09|   Product A|       3|\n",
      "|            10|        103|   2023-06-10|   Product B|       3|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-06-21 14:56:37.545                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | DELETE                                                                                                                                                                                                                           \n",
      " operationParameters | {predicate -> [\"(customer_id#774 = 101)\"]}                                                                                                                                                                                       \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | 1                                                                                                                                                                                                                                \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numRemovedFiles -> 1, numRemovedBytes -> 1734, numCopiedRows -> 7, numAddedChangeFiles -> 0, executionTimeMs -> 2141, numDeletedRows -> 3, scanTimeMs -> 1837, numAddedFiles -> 1, numAddedBytes -> 1688, rewriteTimeMs -> 303} \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-06-21 14:56:20.938                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | UPDATE                                                                                                                                                                                                                           \n",
      " operationParameters | {predicate -> [\"(product_name#776 = Product A)\"]}                                                                                                                                                                                \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | 0                                                                                                                                                                                                                                \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numRemovedFiles -> 1, numRemovedBytes -> 1740, numCopiedRows -> 6, numAddedChangeFiles -> 0, executionTimeMs -> 2181, scanTimeMs -> 1590, numAddedFiles -> 1, numUpdatedRows -> 4, numAddedBytes -> 1734, rewriteTimeMs -> 583} \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                                                                \n",
      " timestamp           | 2023-06-21 14:55:41.709                                                                                                                                                                                                          \n",
      " userId              | null                                                                                                                                                                                                                             \n",
      " userName            | null                                                                                                                                                                                                                             \n",
      " operation           | WRITE                                                                                                                                                                                                                            \n",
      " operationParameters | {mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                           \n",
      " job                 | null                                                                                                                                                                                                                             \n",
      " notebook            | null                                                                                                                                                                                                                             \n",
      " clusterId           | null                                                                                                                                                                                                                             \n",
      " readVersion         | null                                                                                                                                                                                                                             \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                     \n",
      " isBlindAppend       | false                                                                                                                                                                                                                            \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 1740}                                                                                                                                                                     \n",
      " userMetadata        | null                                                                                                                                                                                                                             \n",
      " engineInfo          | Apache-Spark/3.4.0 Delta-Lake/2.4.0                                                                                                                                                                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transactions\").show()\n",
    "delta_table.history().show(truncate=False,vertical=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:43.259576Z",
     "end_time": "2023-06-21T14:56:45.214271Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform an merge operation\n",
    "1. Create a new dataframe that reads the original transaction data from the CSV file\n",
    "2. Perform a merge on the detatable and the new dataframe\n",
    "3. When a match is found, update the quantity for that row to 0\n",
    "4. When no match is found, insert the row from the new dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# Step 6: Perform an upsert\n",
    "original_df =spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(transaction_data_path)\n",
    "original_df.createOrReplaceTempView(\"original_transactions\")\n",
    "\n",
    "spark.sql(\"MERGE INTO transactions AS target \\\n",
    "          using original_transactions AS source ON target.transaction_id = source.transaction_id \\\n",
    "          WHEN MATCHED THEN UPDATE SET quantity = 0 \\\n",
    "          WHEN NOT MATCHED THEN INSERT *\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:56:55.163075Z",
     "end_time": "2023-06-21T14:57:02.412077Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Show the detla table and detla table history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             1|        101|   2023-06-01|   Product A|       2|\n",
      "|             2|        102|   2023-06-02|   Product B|       5|\n",
      "|             3|        103|   2023-06-03|   Product A|       3|\n",
      "|             4|        101|   2023-06-04|   Product C|       1|\n",
      "|             5|        104|   2023-06-05|   Product B|       4|\n",
      "|             6|        102|   2023-06-06|   Product A|       2|\n",
      "|             7|        103|   2023-06-07|   Product C|       3|\n",
      "|             8|        101|   2023-06-08|   Product B|       5|\n",
      "|             9|        104|   2023-06-09|   Product A|       2|\n",
      "|            10|        103|   2023-06-10|   Product B|       3|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n",
      "-RECORD 0-----------------------------------\n",
      " version             | 3                    \n",
      " timestamp           | 2023-06-21 14:56:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | MERGE                \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 2                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numTargetRowsCop... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 1-----------------------------------\n",
      " version             | 2                    \n",
      " timestamp           | 2023-06-21 14:56:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | DELETE               \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 1                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numRemovedFiles ... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 2-----------------------------------\n",
      " version             | 1                    \n",
      " timestamp           | 2023-06-21 14:56:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | UPDATE               \n",
      " operationParameters | {predicate -> [\"(... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | 0                    \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numRemovedFiles ... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "-RECORD 3-----------------------------------\n",
      " version             | 0                    \n",
      " timestamp           | 2023-06-21 14:55:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | WRITE                \n",
      " operationParameters | {mode -> Overwrit... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | null                 \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | false                \n",
      " operationMetrics    | {numFiles -> 1, n... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.4.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delta_table.toDF().show()\n",
    "delta_table.history().show(vertical=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T14:57:27.789652Z",
     "end_time": "2023-06-21T14:57:28.792443Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform a select on the delta table on the second version\n",
    "Use option versionAsOf when reading (spark.read) the delta table from disk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------------+--------+\n",
      "|transaction_id|customer_id|purchase_date|product_name|quantity|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "|             2|        102|   2023-06-02|   Product B|       5|\n",
      "|             3|        103|   2023-06-03|   Product A|       4|\n",
      "|             5|        104|   2023-06-05|   Product B|       4|\n",
      "|             6|        102|   2023-06-06|   Product A|       3|\n",
      "|             7|        103|   2023-06-07|   Product C|       3|\n",
      "|             9|        104|   2023-06-09|   Product A|       3|\n",
      "|            10|        103|   2023-06-10|   Product B|       3|\n",
      "+--------------+-----------+-------------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"./spark-warehouse/transaction_data_delta\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-21T15:14:29.919517Z",
     "end_time": "2023-06-21T15:14:32.304491Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
