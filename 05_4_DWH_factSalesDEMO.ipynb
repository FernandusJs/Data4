{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:32:44.795447300Z",
     "start_time": "2024-09-24T09:32:44.450676800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x207934cd510>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>factSales</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = cc.startLocalCluster(\"factSales\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:33:20.332831900Z",
     "start_time": "2024-09-24T09:32:46.385580100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fact transformations\n",
    "This notebooks creates the sales fact table from scratch based on the operational source table \"sales\"\n",
    "When creating a fact table always follow the listed steps in order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 1 READ NECESSARY SOURCE TABLE(S) AND PERFORM TRANSFORMATIONS\n",
    "**When reading from the source table make sure you include all data necessary:**\n",
    "- to calculate the measure values\n",
    "- the source table keys that you have to use to lookup the correct surrogate keys in the dimension tables.\n",
    "\n",
    "**If more than one table is needed to gather the necesary information you can opt for one of two strategies:**\n",
    "- Use a select query when reading from the jdbc source with the spark.read operation. Avoid complex queries because the operational database needs a lot of resources to run those queries.\n",
    "- Perform a spark.read operation for each table separately and join the tables within Spark. The joins will take place on the cluster instead of the database. You limit the database recources used, but there can be a significant overhead of unnecessary data tranferred to the cluster.\n",
    "\n",
    "\n",
    "In this case we just rename Amount and create a default count_mv column.\n",
    "The transformations are minimal. In reality, transformations can be far more complex. If so, it can be advisable to work out the transforms in more then one step.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----------+\n",
      "|order_id|         order_date|salesrepid|    amount|\n",
      "+--------+-------------------+----------+----------+\n",
      "|       1|2010-10-13 00:00:00|         1| 851804379|\n",
      "|       2|2012-10-01 00:00:00|         1| 683057055|\n",
      "|       3|2011-07-10 00:00:00|         1|1732115679|\n",
      "|       4|2010-08-28 00:00:00|         1|1275042249|\n",
      "|       5|2011-06-17 00:00:00|         1| 694153767|\n",
      "|       6|2011-03-24 00:00:00|         1|1959464599|\n",
      "|       7|2010-02-26 00:00:00|         1|1170677605|\n",
      "|       8|2010-11-23 00:00:00|         1|1588502393|\n",
      "|       9|2012-06-08 00:00:00|         1|1173163372|\n",
      "|      10|2012-08-04 00:00:00|         1| 788682390|\n",
      "|      11|2011-05-30 00:00:00|         1|1951236590|\n",
      "|      12|2009-11-25 00:00:00|         1| 343432817|\n",
      "|      13|2012-02-14 00:00:00|         1| 340274106|\n",
      "|      14|2012-04-15 00:00:00|         1| 958504424|\n",
      "|      15|2010-03-12 00:00:00|         1|1517930834|\n",
      "|      16|2011-03-09 00:00:00|         1|1030063917|\n",
      "|      17|2012-08-04 00:00:00|         1| 792514869|\n",
      "|      18|2011-05-06 00:00:00|         1|1290134482|\n",
      "|      19|2010-12-23 00:00:00|         1|1914973740|\n",
      "|      20|2010-11-08 00:00:00|         1|1710396343|\n",
      "+--------+-------------------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "#EXTRACT\n",
    "cc.set_connectionProfile(\"tutorial_op\")\n",
    "sale_src_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\", \"(select order_id, order_date, salesrepid, amount from sales) as subq\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"Order_ID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 1000) \\\n",
    "    .load()\\\n",
    "\n",
    "sale_src_df.show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:41:40.814547Z",
     "start_time": "2024-09-24T09:41:26.895604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sale_src_df.createOrReplaceTempView(\"sales_source\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:44:16.528173800Z",
     "start_time": "2024-09-24T09:44:16.423221100Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 2 MAKE DIMENSION TABLES AVAILABLE AS VIEWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#EXTRACT\n",
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "dim_salesrep = spark.read.format(\"delta\").load(\"spark-warehouse/dimsalesrep/\")\n",
    "dim_salesrep.createOrReplaceTempView(\"dimSalesRep\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:43:34.193917400Z",
     "start_time": "2024-09-24T09:43:30.134133800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3 Build the fact table\n",
    "\n",
    "Within the creation of a fact table always perform these two tasks:\n",
    "1.   Include the measures of the fact\n",
    "2.   Use the dimension tables to look up the surrogate keys that correspond with the natural key value. In case of SCD2 dimension use the scd_start en scd_end to find the correct version of the data in the dimension\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_SK: long (nullable = true)\n",
      " |-- dateInt: integer (nullable = true)\n",
      " |-- CalendarDate: date (nullable = true)\n",
      " |-- CalendarYear: integer (nullable = true)\n",
      " |-- CalendarMonth: string (nullable = true)\n",
      " |-- MonthOfYear: integer (nullable = true)\n",
      " |-- CalendarDay: string (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DayOfWeekStartMonday: integer (nullable = true)\n",
      " |-- IsWeekDay: string (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- IsLastDayOfMonth: string (nullable = true)\n",
      " |-- DayOfYear: integer (nullable = true)\n",
      " |-- WeekOfYearIso: integer (nullable = true)\n",
      " |-- QuarterOfYear: integer (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "dim_date.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:46:53.367588100Z",
     "start_time": "2024-09-24T09:46:53.278925800Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- salesrepid: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "+--------+-------------------+----------+----------+\n",
      "|order_id|         order_date|salesrepid|    amount|\n",
      "+--------+-------------------+----------+----------+\n",
      "|       1|2010-10-13 00:00:00|         1| 851804379|\n",
      "|       2|2012-10-01 00:00:00|         1| 683057055|\n",
      "|       3|2011-07-10 00:00:00|         1|1732115679|\n",
      "|       4|2010-08-28 00:00:00|         1|1275042249|\n",
      "|       5|2011-06-17 00:00:00|         1| 694153767|\n",
      "+--------+-------------------+----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "sale_src_df.printSchema()\n",
    "sale_src_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T09:47:39.977713200Z",
     "start_time": "2024-09-24T09:47:37.866147Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+--------+----------+--------------------+\n",
      "|Order_ID|date_SK|         salesrep_SK|count_MV|revenue_MV|                 md5|\n",
      "+--------+-------+--------------------+--------+----------+--------------------+\n",
      "|       1|    650|d83fc5ef-6592-4c7...|       1| 851804379|a36d84edbd018a3a9...|\n",
      "|       2|   1369|d83fc5ef-6592-4c7...|       1| 683057055|3cb2b90afa409a2fc...|\n",
      "|       3|    920|d83fc5ef-6592-4c7...|       1|1732115679|cc505b78f8070b46f...|\n",
      "|       4|    604|d83fc5ef-6592-4c7...|       1|1275042249|bc03a8b6d45b197be...|\n",
      "|       5|    897|d83fc5ef-6592-4c7...|       1| 694153767|d89eb2886a3087753...|\n",
      "+--------+-------+--------------------+--------+----------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORM\n",
    "\n",
    "#TODO: Buil the fact table based on the source table and the dimension tables\n",
    "salesFactFromSource = spark.sql(\"select src.Order_ID as Order_ID, dd.date_SK as date_SK, ds.salesrep_SK as salesrep_SK, 1 as count_MV, src.amount as revenue_MV, md5(concat(src.Order_ID,dd.date_SK,ds.salesrep_SK, 1, src.amount)) as md5  from sales_source as src \\\n",
    "                                    left outer join dimdate as dd on cast(src.order_date as DATE) = dd.CalendarDate \\\n",
    "                                    left outer join dimsalesrep as ds on \\\n",
    "                                        src.SalesRepId = ds.salesRepId \\\n",
    "                                        and src.Order_Date > ds.scd_start \\\n",
    "                                        and src.Order_Date <= ds.scd_end\")\n",
    "                                    \n",
    "salesFactFromSource.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T10:01:56.881363300Z",
     "start_time": "2024-09-24T10:01:51.383927700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial load\n",
    "The first time loading the fact table perform a FULL load. All data is written to the Delta Table.\n",
    "After initial load the code line has to be disabled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "salesFactFromSource.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factSales\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T10:03:06.412288400Z",
     "start_time": "2024-09-24T10:02:49.124353900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incremental load\n",
    "When previous runs where performend you can opt for a 'faster' incremental run that only writes away changes. UPDATES and INSERTS are performed in one run.\n",
    "In our solution we use an md5 based on all fields in the source table to detect changes. This is not the most efficient way to detect changes. A better way is to use a timestamp field in the source table and use that to detect changes. This is not implemented in this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt_factSales = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factsales\")\n",
    "dt_factSales.toDF().createOrReplaceTempView(\"factSales_current\")\n",
    "result = spark.sql(\"MERGE INTO factSales_current AS target \\\n",
    "          using factSales_new AS source ON target.orderID = source.orderID \\\n",
    "          WHEN MATCHED and source.MD5<>target.MD5 THEN UPDATE SET * \\\n",
    "          WHEN NOT MATCHED THEN INSERT *\")\n",
    "\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMPORTANT: ALWAYS TEST THE CREATED CODE.\n",
    "# In this example I changed order 498 in the operational database and checked the change after the run.\n",
    "# spark.sql(\"select * from factsales f join dimsalesrep ds on f.salesrepSK = ds.salesrepSK where OrderID = 192  \").show()\n",
    "spark.sql(\"select count(*) from factsales\").show()\n",
    "spark.sql(\"select * from factsales where orderId=1\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the history of your delta fact table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The history information is derived from the delta table log files. They contain a lot of information of all the actions performed on the table. In this case it tells us something about de merge operations. You can find statistics about the update and insert counts in the document.\n",
    "\n",
    "fact.history().show(10,False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
