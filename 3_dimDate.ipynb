{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the needed modules\n",
    "Run this. All modules should be installed (if not, install all packages from requirments.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Create a local Spark-session\n",
    "Several things can go wrong:\n",
    "If error on build.getOrCreate():\n",
    "    Check your environment variabels SPARK_HOME and PATH. They have to refer to the spark installation\n",
    "If error on configure_spark_with_delta_pip\n",
    "    Problem is related to the delta-spark installation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def LDA_coefficients(X,lda):\n",
    "    nb_col = X.shape[1]\n",
    "    matrix= np.zeros((nb_col+1,nb_col), dtype=int)\n",
    "    Z=pd.DataFrame(data=matrix,columns=X.columns)\n",
    "    for j in range(0,nb_col):\n",
    "        Z.iloc[j,j] = 1\n",
    "    LD = lda.transform(Z)\n",
    "    nb_funct= LD.shape[1]\n",
    "    resultaat = pd.DataFrame();\n",
    "    index = ['const']\n",
    "    for j in range(0,LD.shape[0]-1):\n",
    "        index = np.append(index,'C'+str(j+1))\n",
    "    for i in range(0,LD.shape[1]):\n",
    "        coef = [LD[-1][i]]\n",
    "        for j in range(0,LD.shape[0]-1):\n",
    "            coef = np.append(coef,LD[j][i]-LD[-1][i])\n",
    "        result = pd.Series(coef)\n",
    "        result.index = index\n",
    "        column_name = 'LD' + str(i+1)\n",
    "        resultaat[column_name] = result\n",
    "    return resultaat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1952688b940>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://AKDGPORT11191.mshome.net:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DimDate</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DimDate\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "#When shuffled 4 concurrent tasks will be created.\n",
    "builder = configure_spark_with_delta_pip(builder)\n",
    "spark = builder.getOrCreate()\n",
    "builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Reading txt file  with spark-sql\n",
    "If this runs, Spark is working correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'split(split(value, ',', -1), ' ', -1)' due to data type mismatch: argument 1 requires string type, however, 'split(value, ',', -1)' is of array<string> type.; line 1 pos 8;\n'Project [unresolvedalias('explode(split(split(value#1230, ,, -1),  , -1)), Some(org.apache.spark.sql.Column$$Lambda$2112/0x0000000101114840@5f49a810))]\n+- Relation [value#1230] text\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 10\u001B[0m\n\u001B[0;32m      5\u001B[0m file_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(file_type)  \\\n\u001B[0;32m      8\u001B[0m   \u001B[38;5;241m.\u001B[39mload(file_location)\n\u001B[1;32m---> 10\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselectExpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexplode(split(split(value,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m), \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m))\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol != \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m words\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m lowerwords \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect lower(col), count(*) as word from words group by lower(col) order by word desc limit 20\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2048\u001B[0m, in \u001B[0;36mDataFrame.selectExpr\u001B[1;34m(self, *expr)\u001B[0m\n\u001B[0;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(expr) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(expr[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m   2047\u001B[0m     expr \u001B[38;5;241m=\u001B[39m expr[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[1;32m-> 2048\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselectExpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2049\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: cannot resolve 'split(split(value, ',', -1), ' ', -1)' due to data type mismatch: argument 1 requires string type, however, 'split(value, ',', -1)' is of array<string> type.; line 1 pos 8;\n'Project [unresolvedalias('explode(split(split(value#1230, ,, -1),  , -1)), Some(org.apache.spark.sql.Column$$Lambda$2112/0x0000000101114840@5f49a810))]\n+- Relation [value#1230] text\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# File location and type\n",
    "file_location = \"./FileStore/tables/shakespeare.txt\"\n",
    "file_type = \"text\"\n",
    "\n",
    "df = spark.read.format(file_type)  \\\n",
    "  .load(file_location)\n",
    "\n",
    "words = df.selectExpr(\"explode(split(value, ' '))\").filter(\"col != ''\")\n",
    "words.createOrReplaceTempView(\"words\")\n",
    "lowerwords = spark.sql(\"select lower(col), count(*) as word from words group by lower(col) order by word desc limit 20\")\n",
    "lowerwords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing to Delta-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "wordCounts = spark.sql(\"select lower(col) as word, count(*) as amount from words group by lower(col) order by word\")\n",
    "wordCounts.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"wordCounts\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTSTUFF\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nSyntax error at or near 'GENERATED'(line 1, pos 43)\n\n== SQL ==\nCREATE OR REPLACE TABLE demo (   id BIGINT GENERATED ALWAYS AS IDENTITY,   product_type STRING,   sales BIGINT) );\n-------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParseException\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCREATE OR REPLACE TABLE demo ( \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;43m  id BIGINT GENERATED ALWAYS AS IDENTITY, \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;43m  product_type STRING, \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;43m  sales BIGINT) \u001B[39;49m\u001B[38;5;130;43;01m\\\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;43m);\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\pyspark\\sql\\session.py:1034\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[1;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[0;32m   1032\u001B[0m     sqlQuery \u001B[38;5;241m=\u001B[39m formatter\u001B[38;5;241m.\u001B[39mformat(sqlQuery, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1034\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mC:\\DevApps\\DeltaSpark\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mParseException\u001B[0m: \nSyntax error at or near 'GENERATED'(line 1, pos 43)\n\n== SQL ==\nCREATE OR REPLACE TABLE demo (   id BIGINT GENERATED ALWAYS AS IDENTITY,   product_type STRING,   sales BIGINT) );\n-------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "DeltaTable.create(spark) \\\n",
    "  .tableName(\"default.people10m\") \\\n",
    "  .addColumn(\"id\", \"INT\") \\\n",
    "  .addColumn(\"firstName\", \"STRING\") \\\n",
    "  .addColumn(\"middleName\", \"STRING\") \\\n",
    "  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "  .addColumn(\"gender\", \"STRING\") \\\n",
    "  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "  .addColumn(\"dateOfBirth\", DateType(), generatedAlwaysAs=\"CAST(birthDate AS DATE)\") \\\n",
    "  .addColumn(\"ssn\", \"STRING\") \\\n",
    "  .addColumn(\"salary\", \"INT\") \\\n",
    "  .execute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create all typical dimension fields\n",
    "Because we want to represent the date in different ways (weekday, month...), we have to do several tranformations. You can see an extract of our go\n",
    "```\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|dateSK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
    "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
    "|     0|20090101|  2009-01-01|        2009|      January|          1|   Thursday|        5|                   4|        Y|         1|               N|        1|            1|            1|\n",
    "|     1|20090102|  2009-01-02|        2009|      January|          1|     Friday|        6|                   5|        Y|         2|               N|        2|            1|            1|\n",
    "|     2|20090103|  2009-01-03|        2009|      January|          1|   Saturday|        7|                   6|        N|         3|               N|        3|            1|            1|\n",
    "|     3|20090104|  2009-01-04|        2009|      January|          1|     Sunday|        1|                   7|        N|         4|               N|        4|            1|            1|\n",
    "```\n",
    "\n",
    "### Method a: Use spark.sql to perform all the transformations with the help of a sql-query.\n",
    "For many, creating an SQL-select statement is the most easy way to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|dateSK| dateInt|CalendarDate|CalendarYear|CalendarMonth|MonthOfYear|CalendarDay|DayOfWeek|DayOfWeekStartMonday|IsWeekDay|DayOfMonth|IsLastDayOfMonth|DayOfYear|WeekOfYearIso|QuarterOfYear|\n",
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "|     0|20090101|  2009-01-01|        2009|      January|          1|   Thursday|        5|                   4|        Y|         1|               N|        1|            1|            1|\n",
      "|     1|20090102|  2009-01-02|        2009|      January|          1|     Friday|        6|                   5|        Y|         2|               N|        2|            1|            1|\n",
      "|     2|20090103|  2009-01-03|        2009|      January|          1|   Saturday|        7|                   6|        N|         3|               N|        3|            1|            1|\n",
      "|     3|20090104|  2009-01-04|        2009|      January|          1|     Sunday|        1|                   7|        N|         4|               N|        4|            1|            1|\n",
      "|     4|20090105|  2009-01-05|        2009|      January|          1|     Monday|        2|                   1|        Y|         5|               N|        5|            2|            1|\n",
      "|     5|20090106|  2009-01-06|        2009|      January|          1|    Tuesday|        3|                   2|        Y|         6|               N|        6|            2|            1|\n",
      "|     6|20090107|  2009-01-07|        2009|      January|          1|  Wednesday|        4|                   3|        Y|         7|               N|        7|            2|            1|\n",
      "|     7|20090108|  2009-01-08|        2009|      January|          1|   Thursday|        5|                   4|        Y|         8|               N|        8|            2|            1|\n",
      "|     8|20090109|  2009-01-09|        2009|      January|          1|     Friday|        6|                   5|        Y|         9|               N|        9|            2|            1|\n",
      "|     9|20090110|  2009-01-10|        2009|      January|          1|   Saturday|        7|                   6|        N|        10|               N|       10|            2|            1|\n",
      "|    10|20090111|  2009-01-11|        2009|      January|          1|     Sunday|        1|                   7|        N|        11|               N|       11|            2|            1|\n",
      "|    11|20090112|  2009-01-12|        2009|      January|          1|     Monday|        2|                   1|        Y|        12|               N|       12|            3|            1|\n",
      "|    12|20090113|  2009-01-13|        2009|      January|          1|    Tuesday|        3|                   2|        Y|        13|               N|       13|            3|            1|\n",
      "|    13|20090114|  2009-01-14|        2009|      January|          1|  Wednesday|        4|                   3|        Y|        14|               N|       14|            3|            1|\n",
      "|    14|20090115|  2009-01-15|        2009|      January|          1|   Thursday|        5|                   4|        Y|        15|               N|       15|            3|            1|\n",
      "|    15|20090116|  2009-01-16|        2009|      January|          1|     Friday|        6|                   5|        Y|        16|               N|       16|            3|            1|\n",
      "|    16|20090117|  2009-01-17|        2009|      January|          1|   Saturday|        7|                   6|        N|        17|               N|       17|            3|            1|\n",
      "|    17|20090118|  2009-01-18|        2009|      January|          1|     Sunday|        1|                   7|        N|        18|               N|       18|            3|            1|\n",
      "|    18|20090119|  2009-01-19|        2009|      January|          1|     Monday|        2|                   1|        Y|        19|               N|       19|            4|            1|\n",
      "|    19|20090120|  2009-01-20|        2009|      January|          1|    Tuesday|        3|                   2|        Y|        20|               N|       20|            4|            1|\n",
      "+------+--------+------------+------------+-------------+-----------+-----------+---------+--------------------+---------+----------+----------------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimDate = spark.sql(\"select dateSK, \\\n",
    "  year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) as dateInt, \\\n",
    "  CalendarDate, \\\n",
    "  year(calendarDate) AS CalendarYear, \\\n",
    "  date_format(calendarDate, 'MMMM') as CalendarMonth, \\\n",
    "  month(calendarDate) as MonthOfYear, \\\n",
    "  date_format(calendarDate, 'EEEE') as CalendarDay, \\\n",
    "  dayofweek(calendarDate) AS DayOfWeek, \\\n",
    "  weekday(calendarDate) + 1 as DayOfWeekStartMonday, \\\n",
    "  case \\\n",
    "    when weekday(calendarDate) < 5 then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsWeekDay, \\\n",
    "  dayofmonth(calendarDate) as DayOfMonth, \\\n",
    "  case \\\n",
    "    when calendarDate = last_day(calendarDate) then 'Y' \\\n",
    "    else 'N' \\\n",
    "  end as IsLastDayOfMonth, \\\n",
    "  dayofyear(calendarDate) as DayOfYear, \\\n",
    "  weekofyear(calendarDate) as WeekOfYearIso, \\\n",
    "  quarter(calendarDate) as QuarterOfYear \\\n",
    "from  \\\n",
    "  neededDates \\\n",
    "order by \\\n",
    "  calendarDate\")\n",
    "\n",
    "dimDate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```spark.sql``` is used to create the select query and returns the desired DataFrame. Remember to look-up the possible functions [here](https://spark.apache.org/docs/latest/api/sql/).\n",
    "* ```dimDate.show()``` s used to show the records in a DataFrame. Use it during development, but disable when not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Method b: Use the dataframe API\n",
    "\n",
    "This method does not use the sql-like language. You can achieve the same with this method and you get better code completion. See [DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)\n",
    "As an example two columns where added with   ```withColumn``` .\n",
    "```Ã¨xpr()``` is used to write a snippet of 'sql' code and parse it into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----+-------+--------------+\n",
      "|calendarDate|dateSK|year|  month|lasyDayOfMonth|\n",
      "+------------+------+----+-------+--------------+\n",
      "|  2009-01-01|     0|2009|January|             N|\n",
      "|  2009-01-02|     1|2009|January|             N|\n",
      "|  2009-01-03|     2|2009|January|             N|\n",
      "|  2009-01-04|     3|2009|January|             N|\n",
      "|  2009-01-05|     4|2009|January|             N|\n",
      "|  2009-01-06|     5|2009|January|             N|\n",
      "|  2009-01-07|     6|2009|January|             N|\n",
      "|  2009-01-08|     7|2009|January|             N|\n",
      "|  2009-01-09|     8|2009|January|             N|\n",
      "|  2009-01-10|     9|2009|January|             N|\n",
      "|  2009-01-11|    10|2009|January|             N|\n",
      "|  2009-01-12|    11|2009|January|             N|\n",
      "|  2009-01-13|    12|2009|January|             N|\n",
      "|  2009-01-14|    13|2009|January|             N|\n",
      "|  2009-01-15|    14|2009|January|             N|\n",
      "|  2009-01-16|    15|2009|January|             N|\n",
      "|  2009-01-17|    16|2009|January|             N|\n",
      "|  2009-01-18|    17|2009|January|             N|\n",
      "|  2009-01-19|    18|2009|January|             N|\n",
      "|  2009-01-20|    19|2009|January|             N|\n",
      "+------------+------+----+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import explode, expr, sequence,col, date_format\n",
    "df_SparkSQL = df_SQL \\\n",
    "    .withColumn(\"year\", date_format(col(\"calendarDate\"),'yyyy')) \\\n",
    "    .withColumn(\"month\", date_format(col(\"calendarDate\"),'MMMM')) \\\n",
    "    .withColumn(\"lasyDayOfMonth\" \\\n",
    "                ,expr(\"case when calendarDate = last_day(calendarDate) then 'Y' \\\n",
    "                else 'N' \\\n",
    "                end as IsLastDayOfMonth\"))\n",
    "df_SparkSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## TASK:\n",
    "> Complete the transformation in method b until the result matches the result of method a."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Writing the data to a delta-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
